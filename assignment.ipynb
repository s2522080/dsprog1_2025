{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e96801d8",
   "metadata": {},
   "source": [
    "## 最終課題"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "815b9643",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import random\n",
    "from collections import deque\n",
    "from urllib.parse import urljoin, urlparse, urldefrag\n",
    "\n",
    "import requests\n",
    "from requests.adapters import HTTPAdapter\n",
    "from urllib3.util.retry import Retry\n",
    "from bs4 import BeautifulSoup, Comment\n",
    "from urllib import robotparser\n",
    "\n",
    "# 開始URL\n",
    "START_URL = \"https://www.musashino-u.ac.jp/\"\n",
    "DOMAIN = urlparse(START_URL).netloc  # 例: \"www.musashino-u.ac.jp\"\n",
    "\n",
    "# 除外する拡張子（PDFや画像など）\n",
    "EXCLUDE_EXTS = {\".pdf\", \".jpg\", \".jpeg\", \".png\", \".gif\", \".webp\", \".svg\", \".zip\"}\n",
    "\n",
    "# クロール制御パラメータ\n",
    "CONNECT_READ_TIMEOUT = (5, 30)  # 接続5秒、読み取り30秒\n",
    "SLEEP_BETWEEN_REQUESTS = 1.0     # アクセス間隔（秒）\n",
    "RESPECT_ROBOTS = True             # robots.txt を尊重するか\n",
    "MAX_PAGES = 2000                  # 取得ページ数の上限\n",
    "\n",
    "def make_session() -> requests.Session:\n",
    "    \"\"\"\n",
    "    リトライ設定付きの requests.Session を作成。\n",
    "    - 総リトライ8回、接続/読み取り各5回\n",
    "    - backoff_factor=2.0（指数的に待機時間増加）\n",
    "    - 429/5xx を対象\n",
    "    \"\"\"\n",
    "    retry = Retry(\n",
    "        total=8,\n",
    "        connect=5,\n",
    "        read=5,\n",
    "        backoff_factor=2.0,\n",
    "        status_forcelist=[429, 500, 502, 503, 504],\n",
    "        allowed_methods=frozenset([\"GET\", \"HEAD\"]),\n",
    "        raise_on_status=False,\n",
    "        respect_retry_after_header=True,\n",
    "    )\n",
    "    adapter = HTTPAdapter(max_retries=retry, pool_connections=5, pool_maxsize=5)\n",
    "    s = requests.Session()\n",
    "    s.headers.update({\n",
    "        \"User-Agent\": (\n",
    "            \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) \"\n",
    "            \"AppleWebKit/537.36 (KHTML, like Gecko) \"\n",
    "            \"Chrome/124.0.0.0 Safari/537.36\"\n",
    "        ),\n",
    "        \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8\",\n",
    "        \"Accept-Language\": \"ja,en;q=0.8\",\n",
    "        \"Connection\": \"keep-alive\",\n",
    "    })\n",
    "    s.mount(\"https://\", adapter)\n",
    "    s.mount(\"http://\", adapter)\n",
    "    return s\n",
    "\n",
    "# 礼儀的な待機（固定秒＋ジッター）\n",
    "def polite_sleep():\n",
    "    time.sleep(SLEEP_BETWEEN_REQUESTS)\n",
    "\n",
    "# URLが同一ドメインかどうか判定\n",
    "def is_same_domain(url: str) -> bool:\n",
    "    return urlparse(url).netloc == DOMAIN\n",
    "\n",
    "# URLの正規化（相対→絶対、フラグメント削除、拡張子除外など）\n",
    "def normalize_url(base: str, href: str | None) -> str | None:\n",
    "    if not href:\n",
    "        return None\n",
    "    # スキーム除外\n",
    "    if href.startswith((\"javascript:\", \"mailto:\", \"tel:\")):\n",
    "        return None\n",
    "    abs_url = urljoin(base, href)\n",
    "    abs_url, _ = urldefrag(abs_url)\n",
    "\n",
    "    # http を https に統一（可能な場合）\n",
    "    if abs_url.startswith(\"http://\"):\n",
    "        abs_url = abs_url.replace(\"http://\", \"https://\", 1)\n",
    "\n",
    "    # 拡張子で除外（クエリは無視してパスで判定）\n",
    "    path = urlparse(abs_url).path.lower()\n",
    "    for ext in EXCLUDE_EXTS:\n",
    "        if path.endswith(ext):\n",
    "            return None\n",
    "\n",
    "    return abs_url\n",
    "\n",
    "# コメント内リンクを除外して <a href> を抽出\n",
    "def extract_links_excluding_comments(soup: BeautifulSoup, base_url: str) -> set[str]:\n",
    "    # コメントノードを取り除く（コメント内の<a>は対象外）\n",
    "    for comment in soup.find_all(string=lambda text: isinstance(text, Comment)):\n",
    "        comment.extract()\n",
    "\n",
    "    links: set[str] = set()\n",
    "    for a in soup.find_all(\"a\", href=True):\n",
    "        url = normalize_url(base_url, a[\"href\"])\n",
    "        if url:\n",
    "            links.add(url)\n",
    "    return links\n",
    "\n",
    "# <title>抽出（空ならNone）\n",
    "def extract_title(soup: BeautifulSoup) -> str | None:\n",
    "    if soup.title and soup.title.string:\n",
    "        return soup.title.string.strip()\n",
    "    og = soup.find(\"meta\", property=\"og:title\")\n",
    "    if og and og.get(\"content\"):\n",
    "        return og[\"content\"].strip()\n",
    "    h1 = soup.find(\"h1\")\n",
    "    if h1 and h1.get_text(strip=True):\n",
    "        return h1.get_text(strip=True)\n",
    "    return None\n",
    "\n",
    "def build_robot_parser(start_url: str) -> robotparser.RobotFileParser | None:\n",
    "    if not RESPECT_ROBOTS:\n",
    "        return None\n",
    "    rp = robotparser.RobotFileParser()\n",
    "    robots_url = urljoin(start_url, \"/robots.txt\")\n",
    "    try:\n",
    "        rp.set_url(robots_url)\n",
    "        rp.read()\n",
    "    except Exception:\n",
    "        # robots取得に失敗した場合は None（必要なら厳格にブロックする実装へ変更可能）\n",
    "        return None\n",
    "    return rp\n",
    "\n",
    "def can_fetch(url: str, rp: robotparser.RobotFileParser | None) -> bool:\n",
    "    if rp is None or not RESPECT_ROBOTS:\n",
    "        return True\n",
    "    try:\n",
    "        return rp.can_fetch(\"*\", url)\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "def crawl(start_url: str) -> dict[str, str | None]:\n",
    "    site_map: dict[str, str | None] = {}  # key: URL, value: タイトル文字列（HTMLのみ）\n",
    "    visited: set[str] = set()\n",
    "    queue: deque[str] = deque([start_url])\n",
    "\n",
    "    session = make_session()\n",
    "    rp = build_robot_parser(start_url)\n",
    "\n",
    "    pages_crawled = 0\n",
    "\n",
    "    while queue:\n",
    "        current = queue.popleft()\n",
    "\n",
    "        # 正規化（http→https）\n",
    "        if current.startswith(\"http://\"):\n",
    "            current = current.replace(\"http://\", \"https://\", 1)\n",
    "\n",
    "        if current in visited:\n",
    "            continue\n",
    "        if not is_same_domain(current):\n",
    "            continue\n",
    "        if not can_fetch(current, rp):\n",
    "            # robots.txtで禁止ならスキップ\n",
    "            visited.add(current)\n",
    "            continue\n",
    "\n",
    "        # アクセス前に待機（負荷軽減）\n",
    "        polite_sleep()\n",
    "\n",
    "        try:\n",
    "            print(f\"[info] fetching: {current}\", flush=True)\n",
    "            resp = session.get(current, timeout=CONNECT_READ_TIMEOUT)\n",
    "            resp.raise_for_status()\n",
    "        except requests.RequestException as e:\n",
    "            print(f\"[warn] fetch error: {current} -> {e}\")\n",
    "            visited.add(current)\n",
    "            continue\n",
    "\n",
    "        final_url = resp.url\n",
    "        if not is_same_domain(final_url):\n",
    "            visited.add(current)\n",
    "            continue\n",
    "\n",
    "        # Content-TypeでHTML以外を除外\n",
    "        ctype = resp.headers.get(\"Content-Type\", \"\").lower()\n",
    "        if \"text/html\" not in ctype:\n",
    "            visited.add(final_url)\n",
    "            continue\n",
    "\n",
    "        # 文字化け対策: バイト列をそのままBeautifulSoupへ渡す\n",
    "        soup = BeautifulSoup(resp.content, \"lxml\")\n",
    "\n",
    "        # タイトル抽出して辞書へ格納\n",
    "        title = extract_title(soup)\n",
    "        site_map[final_url] = title\n",
    "\n",
    "        visited.add(final_url)\n",
    "        pages_crawled += 1\n",
    "\n",
    "        # MAX_PAGES到達で打ち切り\n",
    "        if pages_crawled >= MAX_PAGES:\n",
    "            print(f\"[info] reached MAX_PAGES={MAX_PAGES}, stopping.\", flush=True)\n",
    "            break\n",
    "\n",
    "        # リンク抽出（コメント除外、同一ドメイン、拡張子除外済み）\n",
    "        links = extract_links_excluding_comments(soup, final_url)\n",
    "        # 既訪問や他ドメインを除外してキューへ\n",
    "        for link in links:\n",
    "            if is_same_domain(link) and link not in visited:\n",
    "                queue.append(link)\n",
    "\n",
    "    return site_map\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    result = crawl(START_URL)\n",
    "    # 表示\n",
    "    for url, title in result.items():\n",
    "        print(f\"{url}\\t{title if title else ''}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
